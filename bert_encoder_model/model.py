import json
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import TransformerEncoder, TransformerEncoderLayer
from transformers import BertTokenizer
from torch.utils.data import Dataset, DataLoader

class TransformerModel(nn.Module):
    def __init__(self, vocab_size, embedding_size, num_heads, hidden_size, num_layers,
                 num_lstm_layers, dropout):
        super(TransformerModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_size)
        self.bert_encoder = BertEncoder(embedding_size, hidden_size)
        self.transformer_encoder = TransformerEncoder(
            TransformerEncoderLayer(hidden_size, num_heads, hidden_size, dropout),
            num_layers
        )
        self.lstm_decoder = LSTMDecoder(hidden_size, hidden_size, num_lstm_layers, dropout)
        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, input):
        embedded = self.embedding(input)
        bert_encoded = self.bert_encoder(embedded)
        transformer_encoded = self.transformer_encoder(bert_encoded)
        lstm_decoded = self.lstm_decoder(transformer_encoded)
        output = self.fc(lstm_decoded)
        return output


class BertEncoder(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(BertEncoder, self).__init__()
        self.fc = nn.Linear(input_size, hidden_size)

    def forward(self, input):
        output = self.fc(input)
        return output


class LSTMDecoder(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, dropout):
        super(LSTMDecoder, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout)

    def forward(self, input):
        output, _ = self.lstm(input)
        return output


# Define your dataset class
class TranslationDataset(Dataset):
    def __init__(self, corpus_file):
        self.data = self.load_corpus(corpus_file)

    def load_corpus(self, corpus_file):
        data = []
        with open(corpus_file, 'r', encoding='utf-8') as file:
            json_data = json.load(file)
            for item in json_data:
                text = item['intent']
                code = item['snippet']
                data.append((text, code))
        return data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text, code = self.data[idx]
        # Perform any necessary tokenization or encoding on text and code
        return text, code


# Inference
def translate_text_to_code(text):
    # Preprocess your input text (e.g., tokenize, convert to tensor)

    # Set your model in evaluation mode
    model.eval()

    # Encode the preprocessed input
    encoded_text, _ = encode_text_and_code(text, [])

    # Convert the encoded_text to a tensor
    encoded_text = torch.tensor(encoded_text)

    # Pass the encoded text through the model
    output = model(encoded_text)

    # Convert the model output to code (e.g., by selecting the most probable tokens)
    code = convert_output_to_code(output)

    return code


def convert_output_to_code(output):
    # Convert the model's output to code based on your specific requirements
    # For example, you can select the most probable token at each timestep

    _, predicted_indices = output.max(dim=2)  # Get the index of the most probable token at each timestep

    # Convert the predicted indices back into code
    code = []
    for indices in predicted_indices:
        code_tokens = [index_to_token[index.item()] for index in indices]  # Convert token indices to actual tokens
        code.append(" ".join(code_tokens))

    return code


def encode_text_and_code(batch_text, batch_code):
    encoded_texts = []
    encoded_codes = []
    for text, code in zip(batch_text, batch_code):
        # Tokenize the text and code
        tokenized_text = tokenizer.tokenize(text)
        tokenized_code = tokenizer.tokenize(code)

        # Encode the tokenized text and code
        encoded_text = tokenizer.convert_tokens_to_ids(tokenized_text)
        encoded_code = tokenizer.convert_tokens_to_ids(tokenized_code)

        encoded_texts.append(encoded_text)
        encoded_codes.append(encoded_code)

    return encoded_texts, encoded_codes

# Instantiate the BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

vocab_size=50000
embedding_size=768
num_heads=4
hidden_size=256
num_layers=6
num_lstm_layers=2
dropout=0.9
learning_rate=0.01
batch_size=10
num_epochs=10

# Define your model
model = TransformerModel(vocab_size, embedding_size, num_heads, hidden_size, num_layers, num_lstm_layers, dropout)

# Define your loss function
criterion = nn.CrossEntropyLoss()

# Define your optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

corpus_file = "/home/gael/Projets/Decoder-ICT-16/WP2/external-knowledge-codegen/data/concode/concode_train.json"
# Prepare your training data
dataset = TranslationDataset(corpus_file)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)


# Training loop
for epoch in range(num_epochs):
    for text, code in dataloader:
        optimizer.zero_grad()

        # Encode text and code
        encoded_text, encoded_code = encode_text_and_code(text, code)

        # Convert encoded_text and encoded_code to tensors
        encoded_text = torch.tensor(encoded_text)
        encoded_code = torch.tensor(encoded_code)

        # Pass the encoded data through the model
        output = model(encoded_text)

        # Compute the loss
        loss = criterion(output, encoded_code)

        # Backpropagation and optimization
        loss.backward()
        optimizer.step()


